while(it<=maxit){
## Step1: sample pi from p(pi|a, x)-------------
for(k in 1L:K){
posterior(obj = cityPrior, ss=citySS[[k]])
PI[k, ] <- rDir(n=1, alpha = cityPrior$gamma$alpha)
posteriorDiscard(obj = cityPrior, ss=citySS[[k]])
}
## calculate the sample mean
if(it>burnin) meanPI <- meanPI+PI[, 1]/(maxit-burnin)
## Step2: sample a from p(a|pi, g)--------------
## use Metropolis-Hastings
a <- MetropolisHastings(nsamples = 1, xini = a, dp=dp, dq=dq, rq=rq)
## increase iteration counter
it <- it+1
}
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates.
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}), type = "h", col = "blue", lwd = 3)
legend(1,  0.005,  legend=c("Sample Mean",  "MLE"), col=c("black",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates.
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}), type = "h", col = "blue", lwd = 3)
legend(1,  0.005,  legend=c("Sample Mean",  "MLE"), col=c("black",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates.
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
type = "h", col = "blue", lwd = 3)
legend(1,  0.005,  legend=c("Poseterior Mean",  "MLE"), col=c("black",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates (number of deaths/total cases).
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
type = "h", col = "blue", lwd = 3)
legend(1,  0.005,  legend=c("Poseterior Mean",  "MLE"),
col=c("black",  "blue"),  lty=c(1, 1),  cex=2, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates (number of deaths/total cases).
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
type = "h", col = "blue", lwd = 3)
points(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
cex=3, col = "blue")
legend(1,  0.005,  legend=c("Poseterior Mean",  "MLE"),
col=c("black",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates (number of deaths/total cases).
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
type = "h", col = "blue", lwd = 3)
points(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
cex=3, pch=21, col = "blue")
legend(1,  0.005,  legend=c("Poseterior Mean",  "MLE"),
col=c("black",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates (number of deaths/total cases).
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3)
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
type = "h", col = "blue", lwd = 3)
points(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
cex=3, pch=19, col = "blue")
legend(1,  0.005,  legend=c("Poseterior Mean",  "MLE"),
col=c("black",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## Step3: plot the result---------------------------------------------
## black bars are the sample mean from the hierarchical Bayesian model
## blue bars are the MLE of the mortality rates (number of deaths/total cases).
plot(1:K, meanPI, type = "h", xlab = "city", ylab = "mortality rate", lwd=3, col = "brown")
points(1:K, meanPI, cex=2, pch=19, col = "brown")
lines(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
type = "h", col = "blue", lwd = 3)
points(1:K+0.2, sapply(cancerData, function(l){sum(l=="death")/length(l)}),
cex=2, pch=19, col = "blue")
legend(1,  0.005,  legend=c("Poseterior Mean",  "MLE"),
col=c("brown",  "blue"),  lty=c(1, 1),  cex=1, lwd = 3)
## The main EM loop
maxit <- 100
## number of EM loops
it <- 1
while(it<=maxit){
## E-step---------------------------------------------------------
## calculate the expected cluster labels: p(z|pi, theta)
for(k in allK) {
z[, k] <- dGaussian(x=mmData, mu = ecMAP[[k]]$muMAP,
Sigma=ecMAP[[k]]$sigmaMAP)+log(mcMAP[k])
}
z <- exp(z-logsumexp(z))
## use logsumexp() to avoid numerical underflow
## calculate the expected sufficient statistics
ssComponents <- lapply(allK, function(k){
sufficientStatistics_Weighted(obj = ec[[k]], x=mmData, w=z[, k])
})
## the expected sufficient statistics of each Gaussian component
## the expected sufficient statistics of the cluster label distribution
ssPi <- sufficientStatistics_Weighted(obj = mc, x=allZ, w=as.vector(z))
## ---------------------------------------------------------
## M-step---------------------------------------------------------
## use the sufficient statistics to update the prior distributions:
## update component distributions
for(k in allK){
posterior(obj = ec[[k]], ss=ssComponents[[k]])
}
## update cluster label distribution
posterior(obj = mc, ss = ssPi)
## calculate the MAP estimates from posterior:
mcMAP <- MAP(mc)
ecMAP <- lapply(ec, MAP)
## Reset the priors for next EM loop-----------------------------------------
## to prepare for the next EM iteration,  discard the sufficient statistics info from the posteriors:
for(k in allK) {
posteriorDiscard(obj = ec[[k]], ss=ssComponents[[k]])
}
posteriorDiscard(obj = mc, ss = ssPi)
## increase the iteration counter
it <- it+1
}
## Get the MAP estimate of pi and theta using EM algorithm.
## load some mixture of Gaussian samples.
## mmData is a numeric matrix with 2 columns,  each row is a sample
## see ?mmData for details
data(mmData)
## number of clusters(mixtures components)
K <- 4L
## the expected cluster label of each observation
z <- matrix(runif(nrow(mmData)*K), nrow(mmData), K)
## temp variable,  all component labels
allK <- 1L:K
## temp variable,  all possible cluster labels for all observations
allZ <- rep(allK, each=nrow(mmData))
## z,  pi and alpha are distributed as a Categorical-Dirichlet sturcture:
mc <- CatDirichlet(gamma = list(alpha=0.5, uniqueLabels=allK))
## create a CatDirichlet object to track the posterior info,  see ?CatDirichlet for details
## each component distribution is a Gaussian-NIW structure:
ec <- replicate(K, GaussianNIW(gamma = list(m=c(0, 0), k=0.00001, v=2, S=diag(2))))
## create a GaussianNIW object to track the posterior info of each mixture component,
## see ?GaussianNIW for details
mcMAP <- MAP(mc)
## initialize the MAP estimate of pi
## initialize the MAP estimate of theta
ecMAP <- replicate(K, list(muMAP=runif(2), sigmaMAP=diag(2)), simplify = FALSE)
## The main EM loop
maxit <- 100
## number of EM loops
it <- 1
while(it<=maxit){
## E-step---------------------------------------------------------
## calculate the expected cluster labels: p(z|pi, theta)
for(k in allK) {
z[, k] <- dGaussian(x=mmData, mu = ecMAP[[k]]$muMAP,
Sigma=ecMAP[[k]]$sigmaMAP)+log(mcMAP[k])
}
z <- exp(z-logsumexp(z))
## use logsumexp() to avoid numerical underflow
## calculate the expected sufficient statistics
ssComponents <- lapply(allK, function(k){
sufficientStatistics_Weighted(obj = ec[[k]], x=mmData, w=z[, k])
})
## the expected sufficient statistics of each Gaussian component
## the expected sufficient statistics of the cluster label distribution
ssPi <- sufficientStatistics_Weighted(obj = mc, x=allZ, w=as.vector(z))
## ---------------------------------------------------------
## M-step---------------------------------------------------------
## use the sufficient statistics to update the prior distributions:
## update component distributions
for(k in allK){
posterior(obj = ec[[k]], ss=ssComponents[[k]])
}
## update cluster label distribution
posterior(obj = mc, ss = ssPi)
## calculate the MAP estimates from posterior:
mcMAP <- MAP(mc)
ecMAP <- lapply(ec, MAP)
## Reset the priors for next EM loop-----------------------------------------
## to prepare for the next EM iteration,  discard the sufficient statistics info from the posteriors:
for(k in allK) {
posteriorDiscard(obj = ec[[k]], ss=ssComponents[[k]])
}
posteriorDiscard(obj = mc, ss = ssPi)
## increase the iteration counter
it <- it+1
}
plot(mmData, col=apply(z, 1, which.max)) #plot the best estimates
mcMAP                                 #the MAP estimate of pi
ecMAP                                 #the MAP estimate of theta_z
plot(mmData, col=apply(z, 1, which.max), pch=19, cex=2) #plot the best estimates
mcMAP                                 #the MAP estimate of pi
ecMAP                                 #the MAP estimate of theta_z
plot(mmData, col=apply(z, 1, which.max), pch=19, cex=2) #plot the best estimates
mcMAP                                 #the MAP estimate of pi
ecMAP                                 #the MAP estimate of theta_z
plot(mmData, col=apply(z, 1, which.max), pch=19, cex=2) #plot the best estimates
## The main EM loop
maxit <- 1000
## number of EM loops
it <- 1
while(it<=maxit){
## E-step---------------------------------------------------------
## calculate the expected cluster labels: p(z|pi, theta)
for(k in allK) {
z[, k] <- dGaussian(x=mmData, mu = ecMAP[[k]]$muMAP,
Sigma=ecMAP[[k]]$sigmaMAP)+log(mcMAP[k])
}
z <- exp(z-logsumexp(z))
## use logsumexp() to avoid numerical underflow
## calculate the expected sufficient statistics
ssComponents <- lapply(allK, function(k){
sufficientStatistics_Weighted(obj = ec[[k]], x=mmData, w=z[, k])
})
## the expected sufficient statistics of each Gaussian component
## the expected sufficient statistics of the cluster label distribution
ssPi <- sufficientStatistics_Weighted(obj = mc, x=allZ, w=as.vector(z))
## ---------------------------------------------------------
## M-step---------------------------------------------------------
## use the sufficient statistics to update the prior distributions:
## update component distributions
for(k in allK){
posterior(obj = ec[[k]], ss=ssComponents[[k]])
}
## update cluster label distribution
posterior(obj = mc, ss = ssPi)
## calculate the MAP estimates from posterior:
mcMAP <- MAP(mc)
ecMAP <- lapply(ec, MAP)
## Reset the priors for next EM loop-----------------------------------------
## to prepare for the next EM iteration,  discard the sufficient statistics info from the posteriors:
for(k in allK) {
posteriorDiscard(obj = ec[[k]], ss=ssComponents[[k]])
}
posteriorDiscard(obj = mc, ss = ssPi)
## increase the iteration counter
it <- it+1
}
mcMAP                                 #the MAP estimate of pi
ecMAP                                 #the MAP estimate of theta_z
plot(mmData, col=apply(z, 1, which.max), pch=19, cex=2) #plot the best estimates
## Sample cluster labels z from DP-MM using Gibbs sampling
library(bbricks)
## load some mixture of Gaussian samples.
## mmData is a numeric matrix with 2 columns,  each row is a sample
## see ?mmData for details
data(mmData)
maxit <- 100                            #number of total samples
burnin <- 50                            #number of burnin samples
## Step1: Initialization -----------------------------------------
obj <- DP(gamma = list(alpha=10, H0aF="GaussianNIW", parH0=list(m=c(0, 0), k=0.001, v=2, S=diag(2)))) #create a DP object to track all the changes,  the DP object in this case is a combination of a CatDP object and a GaussianNIW object
z <- matrix(1L, nrow(mmData), maxit-burnin)    #place-holder for the sampled z
ss <- sufficientStatistics(obj, x=mmData, foreach = TRUE) #sufficient statistics of each observed sample
N <- nrow(mmData)
for(i in 1L:N){ # initialize labels before Gibbs sampling
z[i, 1] <- rPosteriorPredictive(obj = obj, n=1, x=mmData[i, , drop=FALSE])
posterior(obj = obj, ss = ss[[i]],  z = z[i, 1])
}
## Step2: Main Gibbs sampling loop--------------------------------
it <- 1                                 #iteration tracker
pb <- txtProgressBar(min = 0, max = maxit, style = 3)
while(it<=maxit){
if(it>burnin) colIdx <- it-burnin
else colIdx <- 1
for(i in 1L:N){
## remove the sample information from the posterior
posteriorDiscard(obj = obj, ss = ss[[i]], z=z[i, colIdx])
## get a new sample
z[i, colIdx] <- rPosteriorPredictive(obj = obj, n=1, x=mmData[i, , drop=FALSE])
## add the new sample information to the posterior
posterior(obj = obj, ss = ss[[i]], z=z[i, colIdx])
}
if(it>burnin & colIdx<ncol(z)) z[, colIdx+1] <- z[, colIdx] #copy result of previous iteration
it <- it+1
setTxtProgressBar(pb, it)
if(it>=maxit){cat("\n");break}
plot(x=mmData[, 1], y=mmData[, 2], col=z[, colIdx]) #to see how the labels change in each iteration
}
## Step3: Estimate group labels of each observation---------------
## pick the most frequent z as the best estimate
zBest <- apply(z, 1, function(l){
tmp <- table(l)
names(tmp)[which.max(tmp)]
})
while(it<=maxit){
if(it>burnin) colIdx <- it-burnin
else colIdx <- 1
for(i in 1L:N){
## remove the sample information from the posterior
posteriorDiscard(obj = obj, ss = ss[[i]], z=z[i, colIdx])
## get a new sample
z[i, colIdx] <- rPosteriorPredictive(obj = obj, n=1, x=mmData[i, , drop=FALSE])
## add the new sample information to the posterior
posterior(obj = obj, ss = ss[[i]], z=z[i, colIdx])
}
if(it>burnin & colIdx<ncol(z))
z[, colIdx+1] <- z[, colIdx] #copy result of previous iteration
it <- it+1
setTxtProgressBar(pb, it)
if(it>=maxit){cat("\n");break}
## plot(x=mmData[, 1], y=mmData[, 2], col=z[, colIdx]) #to see how the labels change in each iteration
}
## Step3: Estimate group labels of each observation---------------
## pick the most frequent z as the best estimate
zBest <- apply(z, 1, function(l){
tmp <- table(l)
names(tmp)[which.max(tmp)]
})
plot(x=mmData[, 1], y=mmData[, 2], col=zBest)
plot(x=mmData[, 1], y=mmData[, 2], col=zBest, pch=19, cex=2)
plot(x=mmData[, 1], y=mmData[, 2], col=NetworkChange::addTrans(zBest, 50), pch=19, cex=2)
plot(x=mmData[, 1], y=mmData[, 2], col=NetworkChange:::addTrans(zBest, 50), pch=19, cex=2)
plot(x=mmData[, 1], y=mmData[, 2], col=NetworkChange:::addTrans(zBest, 50), pch=19, cex=2)
zBest
plot(x=mmData[, 1], y=mmData[, 2], col=zBest, pch=19, cex=2)
plot(x=mmData[, 1], y=mmData[, 2], col=zBest, pch=19, cex=2)
## Sample cluster labels k from HDP-MM using Gibbs sampling
library(bbricks)
## load some mixture of Gaussian samples.
## mmhData is a list of two elements. mmhData$x is a matrix of Gaussian observations,  each row is an observation; mmhData$groupLabel is the group label of each observation.
## see ?mmhData for details
data(mmhData)
x <- mmhData$x
js <- mmhData$groupLabel
## Sample cluster labels k from HDP-MM using Gibbs sampling
library(bbricks)
## load some mixture of Gaussian samples.
## mmhData is a list of two elements. mmhData$x is a matrix of Gaussian observations,  each row is an observation; mmhData$groupLabel is the group label of each observation.
## see ?mmhData for details
data(mmhData)
x <- mmhData$x
js <- mmhData$groupLabel
js
## Step1: Initialization------------------------------------------
maxit <- 50                             #iterative for maxit times
burnin <- 30                            #number of burn in samples
## create a HDP object to track all the changes,  the HDP object in this case is a combination of a CatHDP object and a GaussianNIW object:
obj <- HDP(gamma = list(gamma=1, j=max(js), alpha=1,
H0aF="GaussianNIW",
parH0=list(m=c(0, 0), k=0.001, v=2, S=diag(2)*0.01)))
ss <- sufficientStatistics(obj$H, x=x, foreach = TRUE) #sufficient statistics
set.seed(1)
z <- rep(1L, nrow(x))
k <- matrix(1L, nrow(x), maxit-burnin)    #place-holder for the sampled k
N <- length(ss)
for(i in 1L:N){# initialize k and z
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, 1] <- tmp["k"]
posterior.HDP(obj = obj, ss = ss[[i]], ss1 = k[i], ss2 = z[i], j = js[i])
}
## Step1: Initialization------------------------------------------
maxit <- 1000                             #iterative for maxit times
burnin <- 100                           #number of burn in samples
## create a HDP object to track all the changes,  the HDP object in this case is a combination of a CatHDP object and a GaussianNIW object:
obj <- HDP(gamma = list(gamma=1, j=max(js), alpha=1,
H0aF="GaussianNIW",
parH0=list(m=c(0, 0), k=0.001, v=2, S=diag(2)*0.01)))
ss <- sufficientStatistics(obj$H, x=x, foreach = TRUE) #sufficient statistics
set.seed(1)
z <- rep(1L, nrow(x))
k <- matrix(1L, nrow(x), maxit-burnin)    #place-holder for the sampled k
N <- length(ss)
for(i in 1L:N){# initialize k and z
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, 1] <- tmp["k"]
posterior.HDP(obj = obj, ss = ss[[i]], ss1 = k[i], ss2 = z[i], j = js[i])
}
## Step2: main Gibbs loop---------------------------------------------
it <- 1                                 #iteration tracker
pb <- txtProgressBar(min = 0, max = maxit, style = 3)
while(it<=maxit){
if(it>burnin) colIdx <- it-burnin
else colIdx <- 1
for(i in 1L:N){
## remove the sample from the posterior info
posteriorDiscard(obj = obj, ss = ss[[i]], ss1=k[i, colIdx], ss2=z[i], j=js[i])
## resample a new partition
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, colIdx] <- tmp["k"]
## add the information of the new sample
posterior(obj = obj, ss = ss[[i]],  ss1=k[i, colIdx], ss2 = z[i], j=js[i])
}
if(it>burnin & colIdx<ncol(k)) k[, colIdx+1] <- k[, colIdx] #copy result of previous iteration
it <- it+1
plot(x=x[, 1], y=x[, 2], col=k[, colIdx])         #to visualize the group label dynamics
setTxtProgressBar(pb, it)
}
## Step2: main Gibbs loop---------------------------------------------
it <- 1                                 #iteration tracker
pb <- txtProgressBar(min = 0, max = maxit, style = 3)
while(it<=maxit){
if(it>burnin) colIdx <- it-burnin
else colIdx <- 1
for(i in 1L:N){
## remove the sample from the posterior info
posteriorDiscard(obj = obj, ss = ss[[i]], ss1=k[i, colIdx], ss2=z[i], j=js[i])
## resample a new partition
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, colIdx] <- tmp["k"]
## add the information of the new sample
posterior(obj = obj, ss = ss[[i]],  ss1=k[i, colIdx], ss2 = z[i], j=js[i])
}
if(it>burnin & colIdx<ncol(k)) k[, colIdx+1] <- k[, colIdx] #copy result of previous iteration
it <- it+1
## plot(x=x[, 1], y=x[, 2], col=k[, colIdx])         #to visualize the group label dynamics
setTxtProgressBar(pb, it)
}
## Step1: Initialization------------------------------------------
maxit <- 1000                             #iterative for maxit times
burnin <- 100                           #number of burn in samples
## create a HDP object to track all the changes,  the HDP object in this case is a combination of a CatHDP object and a GaussianNIW object:
obj <- HDP(gamma = list(gamma=1, j=max(js), alpha=1,
H0aF="GaussianNIW",
parH0=list(m=c(0, 0), k=0.001, v=2, S=diag(2)*0.01)))
ss <- sufficientStatistics(obj$H, x=x, foreach = TRUE) #sufficient statistics
set.seed(1)
z <- rep(1L, nrow(x))
k <- matrix(1L, nrow(x), maxit-burnin)    #place-holder for the sampled k
N <- length(ss)
for(i in 1L:N){# initialize k and z
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, 1] <- tmp["k"]
posterior.HDP(obj = obj, ss = ss[[i]], ss1 = k[i], ss2 = z[i], j = js[i])
}
## Step2: main Gibbs loop---------------------------------------------
it <- 1                                 #iteration tracker
pb <- txtProgressBar(min = 0, max = maxit, style = 3)
while(it<=maxit){
if(it>burnin) colIdx <- it-burnin
else colIdx <- 1
for(i in 1L:N){
## remove the sample from the posterior info
posteriorDiscard(obj = obj, ss = ss[[i]], ss1=k[i, colIdx], ss2=z[i], j=js[i])
## resample a new partition
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, colIdx] <- tmp["k"]
## add the information of the new sample
posterior(obj = obj, ss = ss[[i]],  ss1=k[i, colIdx], ss2 = z[i], j=js[i])
}
if(it>burnin & colIdx<ncol(k)) k[, colIdx+1] <- k[, colIdx] #copy result of previous iteration
it <- it+1
## plot(x=x[, 1], y=x[, 2], col=k[, colIdx])         #to visualize the group label dynamics
setTxtProgressBar(pb, it)
}
## Step1: Initialization------------------------------------------
maxit <- 100                             #iterative for maxit times
burnin <- 20                           #number of burn in samples
## create a HDP object to track all the changes,  the HDP object in this case is a combination of a CatHDP object and a GaussianNIW object:
obj <- HDP(gamma = list(gamma=1, j=max(js), alpha=1,
H0aF="GaussianNIW",
parH0=list(m=c(0, 0), k=0.001, v=2, S=diag(2)*0.01)))
ss <- sufficientStatistics(obj$H, x=x, foreach = TRUE) #sufficient statistics
set.seed(1)
z <- rep(1L, nrow(x))
k <- matrix(1L, nrow(x), maxit-burnin)    #place-holder for the sampled k
N <- length(ss)
for(i in 1L:N){# initialize k and z
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, 1] <- tmp["k"]
posterior.HDP(obj = obj, ss = ss[[i]], ss1 = k[i], ss2 = z[i], j = js[i])
}
## Step2: main Gibbs loop---------------------------------------------
it <- 1                                 #iteration tracker
pb <- txtProgressBar(min = 0, max = maxit, style = 3)
while(it<=maxit){
if(it>burnin) colIdx <- it-burnin
else colIdx <- 1
for(i in 1L:N){
## remove the sample from the posterior info
posteriorDiscard(obj = obj, ss = ss[[i]], ss1=k[i, colIdx], ss2=z[i], j=js[i])
## resample a new partition
tmp <- rPosteriorPredictive(obj = obj, n=1, x=x[i, , drop=FALSE], j=js[i])
z[i] <- tmp["z"]
k[i, colIdx] <- tmp["k"]
## add the information of the new sample
posterior(obj = obj, ss = ss[[i]],  ss1=k[i, colIdx], ss2 = z[i], j=js[i])
}
if(it>burnin & colIdx<ncol(k)) k[, colIdx+1] <- k[, colIdx] #copy result of previous iteration
it <- it+1
## plot(x=x[, 1], y=x[, 2], col=k[, colIdx])         #to visualize the group label dynamics
setTxtProgressBar(pb, it)
}
## Step3: Estimate group labels of each observation---------------
## pick the most frequent k as the best estimate
kBest <- apply(k, 1, function(l){
tmp <- table(l)
names(tmp)[which.max(tmp)]
})
plot(x=x[, 1], y=x[, 2], col=kBest)
## Step3: Estimate group labels of each observation---------------
## pick the most frequent k as the best estimate
kBest <- apply(k, 1, function(l){
tmp <- table(l)
names(tmp)[which.max(tmp)]
})
plot(x=x[, 1], y=x[, 2], col=kBest)
## Step3: Estimate group labels of each observation---------------
## pick the most frequent k as the best estimate
kBest <- apply(k, 1, function(l){
tmp <- table(l)
names(tmp)[which.max(tmp)]
})
plot(x=x[, 1], y=x[, 2], col=kBest, pch=19)
